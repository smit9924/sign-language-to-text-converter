{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1f81fc",
   "metadata": {},
   "source": [
    "## 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35240a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e9f78c",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f52f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f0faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image , model):\n",
    "    image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                     # Image is no longer writable\n",
    "    results = model.process(image)                  # Make prediction\n",
    "    image.flags.writeable = True                     # Image is now writable\n",
    "    image = cv2.cvtColor(image , cv2.COLOR_RGB2BGR)   # COLOR CONVERSION RGB 2 BGR\n",
    "    return image , results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a095b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw styled landmarks to the frame\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Drawing face landmarks \n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    \n",
    "    #Drawing pose landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    \n",
    "    # Drawing left hand landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    \n",
    "    # Drawing right hand landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924dae2a",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7344b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_holistic_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "def extract_left_hand_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh])\n",
    "\n",
    "def extract_right_hand_keypoints(results):\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([rh])\n",
    "\n",
    "def extract_face_keypoints(results):\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([face])\n",
    "\n",
    "def extract_pose_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return np.concatenate([pose])\n",
    "\n",
    "def extract_both_hand_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96ce4a",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3579f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_FOR_BOTH_HAND_KEYPOINTS = os.path.join('Dataset')\n",
    "\n",
    "# Action that we try to detect\n",
    "actions = np.array(['Hey' , 'There' , 'Hello' , 'How' , 'Are' , 'You' , 'I am' , 'Good' , 'Fine' , 'I Love You' , 'Please' , 'Stop This' , 'Stay Here' , 'Help' , 'No' , 'Victory' , 'Walk' , 'Pray'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 10000\n",
    "\n",
    "# Each video will be going to 30 frames in length\n",
    "sequence_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3ba007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folder to store data\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH_FOR_BOTH_HAND_KEYPOINTS, action, str(sequence))) # Directory to store BOTH HAND keypoint\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15e5aa",
   "metadata": {},
   "source": [
    "# 5. Collection Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebfc019",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#save the frame as PNG image\u001b[39;00m\n\u001b[0;32m     19\u001b[0m npy_frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH_FOR_FRAMES, action, \u001b[38;5;28mstr\u001b[39m(sequence), \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(frame_num)))\n\u001b[1;32m---> 20\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpy_frame_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Make detections\u001b[39;00m\n\u001b[0;32m     23\u001b[0m image, results \u001b[38;5;241m=\u001b[39m mediapipe_detection(frame , holistic)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Accessing the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FPS , 23)\n",
    "\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5 , min_tracking_confidence = 0.5) as holistic:\n",
    "    \n",
    "    #looping through action\n",
    "    for action in actions:\n",
    "        # looping through sequences of videos\n",
    "        for sequence in range(no_sequences):\n",
    "            #looing through video length sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame , holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image , results)\n",
    "                \n",
    "                # Apply wait logic\n",
    "                if frame_num == 0 :\n",
    "                    cv2.putText(image, 'START COLLECTION', (120,200),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} video number {}'.format(action, sequence), (15, 12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed' , image)\n",
    "                    # cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {} video number {}'.format(action, sequence), (15, 12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed' , image)\n",
    "                \n",
    "                # Export BOTH HAND keypoints\n",
    "                both_hand_keypoints = extract_both_hand_keypoints(results)\n",
    "                npy_both_hand_path = os.path.join(DATA_PATH_FOR_BOTH_HAND_KEYPOINTS, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_both_hand_path, both_hand_keypoints)\n",
    "                \n",
    "                # Show to the screen\n",
    "                # cv2.imshow('OpenCV Feed' , frame)\n",
    "                \n",
    "    \n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        cv2.waitKey(10000)\n",
    "        \n",
    "# Releasing the web camera\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f62f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Releasing the web camera\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465b6ac7",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Lables and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc460ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c4baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {lable:num for num, lable in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5ccdaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hey': 0,\n",
       " 'There': 1,\n",
       " 'Hello': 2,\n",
       " 'How': 3,\n",
       " 'Are': 4,\n",
       " 'You': 5,\n",
       " 'I am': 6,\n",
       " 'Good': 7,\n",
       " 'Fine': 8,\n",
       " 'I Love You': 9,\n",
       " 'Please': 10,\n",
       " 'Stop This': 11,\n",
       " 'Stay Here': 12,\n",
       " 'Help': 13,\n",
       " 'No': 14,\n",
       " 'Victory': 15,\n",
       " 'Walk': 16,\n",
       " 'Pray': 17}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bad6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH_FOR_BOTH_HAND_KEYPOINTS, action, str(sequence) , \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67dd4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the value of X cordinates\n",
    "x = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6633bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the value of Y cordinates\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "327c4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data for the train and testing\n",
    "# We are testing with 5% of the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03cad48",
   "metadata": {},
   "source": [
    "# 7. Built and Traing LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057d38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM , Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9b92b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0414b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DEEP LEARNING model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(1,126)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9079443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e033323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "3938/3938 [==============================] - 39s 8ms/step - loss: 0.6185 - categorical_accuracy: 0.7679\n",
      "Epoch 2/2000\n",
      "3938/3938 [==============================] - 30s 8ms/step - loss: 0.0896 - categorical_accuracy: 0.9724\n",
      "Epoch 3/2000\n",
      "3938/3938 [==============================] - 30s 8ms/step - loss: 0.0314 - categorical_accuracy: 0.9913\n",
      "Epoch 4/2000\n",
      "3938/3938 [==============================] - 29s 7ms/step - loss: 0.0208 - categorical_accuracy: 0.9939\n",
      "Epoch 5/2000\n",
      "3938/3938 [==============================] - 30s 8ms/step - loss: 0.0155 - categorical_accuracy: 0.9954\n",
      "Epoch 6/2000\n",
      "3938/3938 [==============================] - 30s 8ms/step - loss: 0.0132 - categorical_accuracy: 0.9960\n",
      "Epoch 7/2000\n",
      "3938/3938 [==============================] - 30s 8ms/step - loss: 0.0124 - categorical_accuracy: 0.9965\n",
      "Epoch 8/2000\n",
      " 107/3938 [..............................] - ETA: 29s - loss: 0.0055 - categorical_accuracy: 0.9980"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feada72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 64)             48896     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1, 128)            98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 18)                594       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203,954\n",
      "Trainable params: 203,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048692f",
   "metadata": {},
   "source": [
    "# 8. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c45546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 9s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "res=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9506e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stay Here'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e93145ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stay Here'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47df567",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7141822",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('18_action(word_sentance)_both_hand.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c236dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51299d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('16_action(word_sentance)_both_hand.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a383e06",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confussion Matrix and Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c14d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c395c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 7s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1af6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaac2f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[50950,     0],\n",
       "        [    5,  3045]],\n",
       "\n",
       "       [[50938,     2],\n",
       "        [    0,  3060]],\n",
       "\n",
       "       [[51049,     1],\n",
       "        [   13,  2937]],\n",
       "\n",
       "       [[51083,     2],\n",
       "        [    5,  2910]],\n",
       "\n",
       "       [[50990,     3],\n",
       "        [   14,  2993]],\n",
       "\n",
       "       [[51025,    12],\n",
       "        [   21,  2942]],\n",
       "\n",
       "       [[51001,     1],\n",
       "        [    5,  2993]],\n",
       "\n",
       "       [[51004,     1],\n",
       "        [    4,  2991]],\n",
       "\n",
       "       [[50925,     0],\n",
       "        [    0,  3075]],\n",
       "\n",
       "       [[50964,     3],\n",
       "        [    0,  3033]],\n",
       "\n",
       "       [[50900,    69],\n",
       "        [    0,  3031]],\n",
       "\n",
       "       [[51056,    10],\n",
       "        [    8,  2926]],\n",
       "\n",
       "       [[51013,     5],\n",
       "        [   15,  2967]],\n",
       "\n",
       "       [[51044,     3],\n",
       "        [    6,  2947]],\n",
       "\n",
       "       [[50953,     4],\n",
       "        [   13,  3030]],\n",
       "\n",
       "       [[50990,     1],\n",
       "        [    1,  3008]],\n",
       "\n",
       "       [[50956,     5],\n",
       "        [    1,  3038]],\n",
       "\n",
       "       [[51037,     0],\n",
       "        [   11,  2952]]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9acc761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9977407407407407"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cf10a",
   "metadata": {},
   "source": [
    "# 11. Test in Reallife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1820d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display probability in the realtiome\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245) , (210,100,20), (100,190,50), (245,117,16), (117,245,16), (16,117,245) , (210,100,20), (100,190,50), (245,117,16), (117,245,16), (16,117,245) , (210,100,20), (100,190,50), (245,117,16), (117,245,16), (16,117,245) , (210,100,20), (100,190,50), (245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e498e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Hey\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Stop This\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Stop This\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Please\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Please\n"
     ]
    }
   ],
   "source": [
    "# Import library to convert text into speech\n",
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Import libraries to make async call to the speech function\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# Import library to perform multithreadting\n",
    "import threading\n",
    "\n",
    "# Function that will convert text to the speak\n",
    "def texToSpeech(word):\n",
    "    engine.say(\"hii\")\n",
    "    engine.runAndWait()\n",
    "    \n",
    "\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.95\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_both_hand_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-1:]\n",
    "        \n",
    "        if len(sequence) == 1:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "#                         p = multiprocessing.Process(target=texToSpeech , args = actions[np.argmax(res)])\n",
    "#                         p = multiprocessing.Process(target=texToSpeech)\n",
    "#                         p.start()\n",
    "#                         t = threading.Thread(target = texToSpeech , args = actions[np.argmax(res)])\n",
    "#                         t.start()\n",
    "#                         engine.say(actions[np.argmax(res)])\n",
    "#                         engine.runAndWait()\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "#                     p = multiprocessing.Process(target=texToSpeech , args = actions[np.argmax(res)])\n",
    "#                     p = multiprocessing.Process(target=texToSpeech )\n",
    "#                     p.start()\n",
    "#                     t = threading.Thread(target = texToSpeech , args = actions[np.argmax(res)])\n",
    "#                     t.start()\n",
    "#                     engine.say(actions[np.argmax(res)])\n",
    "#                     engine.runAndWait()\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        scale_percent = 150 # percent of original size\n",
    "        width = int(image.shape[1] * scale_percent / 100)\n",
    "        height = int(image.shape[0] * scale_percent / 100)\n",
    "        dim = (width, height)\n",
    "\n",
    "        # resize image\n",
    "        image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1de5a2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1662)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequence).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4962a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am smit patel\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "def f(x):\n",
    "    print(x*x)\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(processes=1)              # Start a worker processes.\n",
    "    result = pool.apply_async(f, [10]) # Evaluate \"f(10)\" asynchronously calling callback when finished.\n",
    "\n",
    "print(\"I am smit patel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04a0ccd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m*\u001b[39mx\n\u001b[0;32m      6\u001b[0m pool \u001b[38;5;241m=\u001b[39m Pool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)              \u001b[38;5;66;03m# Start a worker processes.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mapply_async(f, [\u001b[38;5;241m10\u001b[39m] , \u001b[43mcallback\u001b[49m) \u001b[38;5;66;03m# Evaluate \"f(10)\" asynchronously calling callback when finished.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am smit patel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'callback' is not defined"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "def f(x):\n",
    "    print(\"hello\")\n",
    "    return x*x\n",
    "\n",
    "pool = Pool(processes=1)              # Start a worker processes.\n",
    "result = pool.apply_async(f, [10] , callback) # Evaluate \"f(10)\" asynchronously calling callback when finished.\n",
    "\n",
    "print(\"I am smit patel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "155c2590",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the square\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     p1 \u001b[38;5;241m=\u001b[39m \u001b[43mProcess\u001b[49m(target \u001b[38;5;241m=\u001b[39m print_square)\n\u001b[0;32m     11\u001b[0m     p2 \u001b[38;5;241m=\u001b[39m Process(target \u001b[38;5;241m=\u001b[39m print_cube)\n\u001b[0;32m     13\u001b[0m     p1\u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Process' is not defined"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "  \n",
    "def print_cube():\n",
    "    print(\"In the cube\")\n",
    "\n",
    "def print_square(num):\n",
    "    print(\"In the square\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    p1 = Process(target = print_square)\n",
    "    p2 = Process(target = print_cube)\n",
    "    \n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    \n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    \n",
    "    print(\"done\")\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4396574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of main process: 10264\n",
      "ID of process p1: 27292\n",
      "ID of process p2: 4808\n",
      "Both processes finished execution!\n",
      "Process p1 is alive: False\n",
      "Process p2 is alive: False\n"
     ]
    }
   ],
   "source": [
    "# importing the multiprocessing module\n",
    "import multiprocessing\n",
    "import os\n",
    "  \n",
    "def worker1():\n",
    "    # printing process id\n",
    "    print(\"ID of process running worker1: {}\".format(os.getpid()))\n",
    "  \n",
    "def worker2():\n",
    "    # printing process id\n",
    "    print(\"ID of process running worker2: {}\".format(os.getpid()))\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    # printing main program process id\n",
    "    print(\"ID of main process: {}\".format(os.getpid()))\n",
    "  \n",
    "    # creating processes\n",
    "    p1 = multiprocessing.Process(target=worker1)\n",
    "    p2 = multiprocessing.Process(target=worker2)\n",
    "  \n",
    "    # starting processes\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "  \n",
    "    # process IDs\n",
    "    print(\"ID of process p1: {}\".format(p1.pid))\n",
    "    print(\"ID of process p2: {}\".format(p2.pid))\n",
    "  \n",
    "    # wait until processes are finished\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "  \n",
    "    # both processes finished\n",
    "    print(\"Both processes finished execution!\")\n",
    "  \n",
    "    # check if processes are alive\n",
    "    print(\"Process p1 is alive: {}\".format(p1.is_alive()))\n",
    "    print(\"Process p2 is alive: {}\".format(p2.is_alive()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55cd29d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (textTospeech):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_12456\\3061053150.py\", line 9, in textTospeech\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyttsx3\\engine.py\", line 177, in runAndWait\n",
      "    raise RuntimeError('run loop already started')\n",
      "RuntimeError: run loop already started\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Thread' object has no attribute 'endLoop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m p \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mtextTospeech, args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTHis is the smit patel\u001b[39m\u001b[38;5;124m'\u001b[39m,))\n\u001b[0;32m     18\u001b[0m p\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m---> 19\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendLoop\u001b[49m()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter thread abcd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Thread' object has no attribute 'endLoop'"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "import threading\n",
    "\n",
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def textTospeech(word):\n",
    "    engine.say(word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def f(name):\n",
    "    print('hello', name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     p = Process(target=f, args=('bob',))\n",
    "#     p = threading.Thread(target=f, args=('bob',))\n",
    "    p = threading.Thread(target=textTospeech, args=('THis is the smit patel',))\n",
    "    p.start()\n",
    "    p.endLoop()\n",
    "    print(\"After thread abcd\")\n",
    "#     p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7493d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Thread' object has no attribute 'endLoop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendLoop\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Thread' object has no attribute 'endLoop'"
     ]
    }
   ],
   "source": [
    "p.endLoop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc620bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
